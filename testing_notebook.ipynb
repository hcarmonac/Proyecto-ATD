{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3eb5703",
   "metadata": {},
   "source": [
    "## **Get the data via API for making the graph (server)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a58c66df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_graph_data(ticker):\n",
    "    \"\"\"\n",
    "    \n",
    "        Obtains the data of the stock price evolution for a given ticker over the last year\n",
    "\n",
    "        Parameters:\n",
    "            ticker (str): Stock ticker symbol\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary containing the data for making the plot with Plotly {dates: [], prices: []}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Variables and API setup\n",
    "    API_KEY = 'ea51535a06ab42f0824812f815f2eb08' \n",
    "    OUTPUT_SIZE = 252\n",
    "    URL = 'https://api.twelvedata.com/time_series'\n",
    "    START_DATE = (date.today() - timedelta(days=365)).isoformat()\n",
    "\n",
    "    params = {\n",
    "        'symbol': ticker,\n",
    "        'interval': '1day',\n",
    "        'outputsize': OUTPUT_SIZE,\n",
    "        'start_date': START_DATE,\n",
    "        'order': 'asc',\n",
    "        'apikey': API_KEY\n",
    "    }\n",
    "    \n",
    "    # Make the API request\n",
    "    try:\n",
    "        response = requests.get(URL, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        values = data['values']\n",
    "        \n",
    "        # Extract dates and prices\n",
    "        dates, prices = [], []\n",
    "        for day in values:\n",
    "            # Para que la fecha tenga el mismo formato que las news y poder plotear\n",
    "            date_obj = datetime.strptime(day['datetime'], '%Y-%m-%d')\n",
    "            dates.append(date_obj)\n",
    "            prices.append(float(day['close']))\n",
    "        \n",
    "        return {'ticker': ticker, 'dates': dates, 'prices': prices}\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df0200",
   "metadata": {},
   "source": [
    "## **Make the graph with the previous data (client)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02f9ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def make_graph(graph_data, news):\n",
    "    \"\"\"\n",
    "    Create a Plotly graph for stock price evolution and save it as an image.\n",
    "    \n",
    "    Parameters:\n",
    "        graph_data (dict): A dictionary containing 'dates' and 'prices' lists.\n",
    "    \n",
    "    Returns:\n",
    "        str: The file path to the saved HTML graph.\n",
    "    \"\"\"\n",
    "    ticker, dates, prices = graph_data['ticker'], graph_data['dates'], graph_data['prices']\n",
    "    \n",
    "    # 1. Plotea el gráfico de la evolución del stock\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dates, \n",
    "        y=prices, \n",
    "        mode='lines', \n",
    "        name=ticker.upper(),\n",
    "        line=dict(color='royalblue', width=2),\n",
    "        hovertemplate='<b>Precio:</b> $%{y:.2f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "    for new in news:\n",
    "        date_new = new[0]\n",
    "        title_new = new[1]\n",
    "        \n",
    "        # Plotea solo las noticias que coinciden con dias en los que la bolsa está abierta\n",
    "        if date_new in dates:\n",
    "            # Plotea las noticias como lineas verticales infinitas\n",
    "            fig.add_vline(\n",
    "                x=date_new, \n",
    "                line_width=1, \n",
    "                line_dash=\"dash\", \n",
    "                line_color=\"grey\"\n",
    "            )\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[date_new],\n",
    "                y=[prices[dates.index(date_new)]],\n",
    "                mode='markers',\n",
    "                marker=dict(size=0, opacity=0),\n",
    "                name='Noticia',\n",
    "                text=[f\"<b>Noticia:</b> {title_new}\"],\n",
    "                hovertemplate='%{text}<extra></extra>',\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title = f'{ticker.upper()} Stock Price Evolution Over the Last Year',\n",
    "        xaxis_title = 'Date',\n",
    "        yaxis_title = 'Closing Price (USD)',\n",
    "        hovermode='x unified',\n",
    "        hoverdistance=5,\n",
    "        template = 'plotly_white',\n",
    "        hoverlabel = dict(bgcolor=\"white\", font_size=13, font_family=\"Rockwell\")\n",
    "    )\n",
    "    \n",
    "    fig.write_html(f\"stock_price_graph.html\")\n",
    "\n",
    "    return \"stock_price_graph.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9bb23d",
   "metadata": {},
   "source": [
    "## **FMD API for getting the analysts opinion (server)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_estimations(ticker):\n",
    "    \"\"\"\n",
    "    \n",
    "        Fetch financial estimations from Financial Modeling Prep API\n",
    "        \n",
    "        Parameters:\n",
    "            ticker: str - The stock ticker symbol to search for.\n",
    "        Returns:\n",
    "            Dictionary with the fetched estimations (key-value pairs including metrics like price target, earnings estimates, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Financial Modeling Prep (FMP) API key for estimations\n",
    "    API_KEY = 'm6B6VyNRoaMYJOIxJPWLzD6K9oVopgoe'\n",
    "    \n",
    "    # Prepare the URLs\n",
    "    URL_FINANCIAL_ESTIMATES = f'https://financialmodelingprep.com/stable/analyst-estimates'\n",
    "    params_financial_estimates = {\n",
    "        'apikey': API_KEY,\n",
    "        'symbol': ticker.upper(),\n",
    "        'period': 'annual'\n",
    "    }\n",
    "    \n",
    "    URL_PRICE_TARGET_CONSENSUS = f'https://financialmodelingprep.com/stable/price-target-consensus'\n",
    "    params_price_target_consensus = {\n",
    "        'apikey': API_KEY,\n",
    "        'symbol': ticker.upper()\n",
    "    }\n",
    "    \n",
    "    URL_STOCK_GRADES = f'https://financialmodelingprep.com/stable/grades-consensus'\n",
    "    params_stock_grades = params_price_target_consensus\n",
    "    \n",
    "    # Function to fetch data\n",
    "    def fetch_data(url, params):\n",
    "        try:\n",
    "            # Make the GET request to the API endpoint\n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Process the response data\n",
    "            if isinstance(data, list):\n",
    "                return data[0] if data else {}\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                return data\n",
    "            \n",
    "            # Handle unexpected data format\n",
    "            print(f\"Unexpected data format from {url}: {type(data)}\")\n",
    "            return {}\n",
    "        \n",
    "        # Handle request exceptions\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred while fetching data from {url}: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    # Fetch data from the three endpoints\n",
    "    financial_estimates = fetch_data(URL_FINANCIAL_ESTIMATES, params_financial_estimates)\n",
    "    price_target_consensus = fetch_data(URL_PRICE_TARGET_CONSENSUS, params_price_target_consensus)\n",
    "    stock_grades = fetch_data(URL_STOCK_GRADES, params_stock_grades)\n",
    "    \n",
    "    # Return combined data\n",
    "    return {**financial_estimates, **price_target_consensus, **stock_grades}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee20f4a",
   "metadata": {},
   "source": [
    "## **Finviz.com webscrape via `Selenium` for obtaing the company information (server)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "def get_information(ticker):\n",
    "    \"\"\"\n",
    "    \n",
    "        Selenium Web Scraping from finviz.com\n",
    "        \n",
    "        Parameters:\n",
    "            ticker: str - The stock ticker symbol to search for.\n",
    "        Returns:\n",
    "            Dictionary with the extracted information (key-value pairs including metrics like P/E ratio, market cap, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Hide warnings from Selenium\n",
    "    options = Options()\n",
    "    options.add_argument('--log-level=3')\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    \n",
    "    # Initialize the WebDriver and information dictionary\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    info = {}\n",
    "    \n",
    "    # Full screen the window\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    try:\n",
    "        # Access the website\n",
    "        driver.get(\"https://finviz.com/\")\n",
    "        \n",
    "        # Handle cookie consent pop-up\n",
    "        time.sleep(1)  # Wait for the pop-up to appear\n",
    "\n",
    "        \"\"\"leer_mas = driver.find_element(By.CSS_SELECTOR, \".Button__StyledButton-buoy__sc-a1qza5-0.elJono\")\n",
    "        if leer_mas:\n",
    "            leer_mas.click()\"\"\"\n",
    "\n",
    "        cookie_reject_button = driver.find_element(By.CLASS_NAME, \"Button__StyledButton-buoy__sc-a1qza5-0\")\n",
    "        cookie_reject_button.click()\n",
    "        \n",
    "        # Locate the search input field and enter the ticker symbol. Then wait 0.5 seconds and press ENTER\n",
    "        search_input = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.TAG_NAME, \"input\")))\n",
    "        search_input.click()\n",
    "        search_input.send_keys(ticker.upper())\n",
    "        time.sleep(0.5)\n",
    "        search_input.send_keys(Keys.ENTER)\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"snapshot-table2\")))\n",
    "        \n",
    "        # Extract the required information\n",
    "        table = driver.find_element(By.CLASS_NAME, \"snapshot-table2\")\n",
    "        rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "        \n",
    "        # Iterate through rows and extract key-value pairs\n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            for i in range(0, len(cells), 2):\n",
    "                key = cells[i].text\n",
    "                value = cells[i + 1].text\n",
    "                info[key] = value\n",
    "    \n",
    "    # Handle exceptions\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    # Ensure the driver is closed properly\n",
    "    finally:\n",
    "        time.sleep(1)\n",
    "        driver.quit()\n",
    "    \n",
    "    # Return the extracted information\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702924d0",
   "metadata": {},
   "source": [
    "## **Elmundo.es webscrape via `Selenium` for getting the latest news (server)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31f365c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import yfinance as yf\n",
    "import re\n",
    "\n",
    "def get_news(ticker):\n",
    "    \"\"\"\n",
    "    \n",
    "    Selenium Web Scraping from elmundo.es for news related to the company represented by the ticker symbol.\n",
    "    \n",
    "    Parameters:\n",
    "        ticker: str - The stock ticker symbol to search for.\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples containing (date, title, link) of relevant news articles.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get company name from ticker\n",
    "    try:\n",
    "        empresa = yf.Ticker(ticker).info['shortName']\n",
    "        empresa = re.search(r'\\w+', empresa).group(0)  # Take only the first word of the company name\n",
    "    except:\n",
    "        empresa = ticker  # If fails, use ticker as company name\n",
    "    \n",
    "    # Hide warnings from Selenium\n",
    "    options = Options()\n",
    "    options.add_argument('--log-level=3')\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    \n",
    "    \n",
    "    # Initialize WebDriver\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get('https://www.elmundo.es/')\n",
    "        driver.maximize_window()\n",
    "    \n",
    "    except:\n",
    "        print(\"Error initializing WebDriver or accessing elmundo.es\")\n",
    "    \n",
    "    # Handle cookies pop-up\n",
    "    try:\n",
    "        cookies_button=WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"ue-accept-notice-button\"))\n",
    "                )\n",
    "        cookies_button.click()\n",
    "    except:\n",
    "        print(\"Error handling cookies pop-up\")\n",
    "    \n",
    "    # Click search button\n",
    "    try:\n",
    "        search_button=WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CLASS_NAME, \"ue-c-main-header__search-box\"))\n",
    "                )\n",
    "        search_button.click()\n",
    "    except:\n",
    "        print(\"Error clicking search button\")\n",
    "    \n",
    "    # Click advanced search\n",
    "    try:\n",
    "        busqueda_avanzada=WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.LINK_TEXT, \"búsqueda avanzada »\"))\n",
    "                )\n",
    "        busqueda_avanzada.click()\n",
    "    except:\n",
    "        print(\"Error clicking advanced search\")\n",
    "        \n",
    "    # Choose 50 results per page\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_all_elements_located((By.CLASS_NAME, 'consejos'))) # Wait for all options to load\n",
    "        desplegables = driver.find_elements(By.CLASS_NAME, 'consejos') # Get all options\n",
    "        select = Select(desplegables[2]) # Take the third\n",
    "        select.select_by_value(\"50\")\n",
    "    except: \n",
    "        print(\"Error selecting the number of results per page\")\n",
    "\n",
    "    # Choose news from the last year\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_all_elements_located((By.CLASS_NAME, 'consejos')) # Wait for all options to load\n",
    "                )\n",
    "        select = Select(desplegables[0]) # Take the first\n",
    "        select.select_by_value(\"365\")\n",
    "    except: \n",
    "        print(\"Error selecting the number of results per page\")\n",
    "        \n",
    "    # Choose 70% match percentage\n",
    "    try:\n",
    "        select = Select(desplegables[3]) # Take the fourth\n",
    "        select.select_by_value(\"70\")\n",
    "    except: \n",
    "        print(\"Error selecting the match percentage\")\n",
    "        \n",
    "    # Send keys of the company name and submit\n",
    "    try:\n",
    "        insertar_nombre=WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.ID, \"q\"))\n",
    "                )\n",
    "        insertar_nombre.send_keys(empresa)\n",
    "        insertar_nombre.send_keys(Keys.ENTER)\n",
    "    \n",
    "    except:\n",
    "        print(\"Error sending keys of the company name\")\n",
    "        \n",
    "    # Click a button to sort news by date\n",
    "    try:\n",
    "        boton_ordenar_fecha=WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.LINK_TEXT, \"Ordenar por FECHA\"))\n",
    "                )\n",
    "        boton_ordenar_fecha.click()\n",
    "    except:\n",
    "        print(\"Error with the button to sort by date\")\n",
    "\n",
    "    # Click on economy to only show economic news\n",
    "    try:\n",
    "        boton_economia=WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.LINK_TEXT, \"Economía\"))\n",
    "                )\n",
    "        boton_economia.click()\n",
    "    except:\n",
    "        print(\"Error with the button for economic news\")\n",
    "    \n",
    "    lista=[]\n",
    "    # Collect data from each page and then click on the next page button if it exists; if it doesn't exist, break the loop\n",
    "    while True:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_all_elements_located((By.TAG_NAME, \"li\"))) # Wait for all li elements to load\n",
    "            \n",
    "            todos_los_li = driver.find_elements(By.TAG_NAME, \"li\")\n",
    "        \n",
    "            # We only save the news that have the company name in their text (title or subtitle)\n",
    "        except:\n",
    "            print(\"Error getting the li elements of the news\")\n",
    "        \n",
    "    # Filter important news and save dates (coherence 80% or more)\n",
    "        try:\n",
    "            for i in todos_los_li:\n",
    "                if empresa.lower() in i.text.lower():\n",
    "                    elemento_a = i.find_element(By.TAG_NAME, 'a')\n",
    "                    titulo = elemento_a.text\n",
    "                    enlace = elemento_a.get_attribute('href')\n",
    "                    fecha_sucia = i.find_element(By.CLASS_NAME, 'fecha').text\n",
    "                    fecha_object = datetime.strptime(fecha_sucia, '%d/%m/%Y')\n",
    "                    lista.append((fecha_object, titulo, enlace))\n",
    "        except:\n",
    "            print(\"Error filtering that the company name is in the title or subtitle and saving it\")\n",
    "            \n",
    "    # Go to the next page    \n",
    "        try:\n",
    "            boton_siguiente_pag=WebDriverWait(driver, 2).until(\n",
    "                        EC.element_to_be_clickable((By.LINK_TEXT, \"Siguiente »\"))\n",
    "            )\n",
    "            \n",
    "            boton_siguiente_pag.click()\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    # Close the driver\n",
    "    time.sleep(1)\n",
    "    driver.quit()\n",
    "    \n",
    "    # Return the list of news\n",
    "    return lista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7aaed",
   "metadata": {},
   "source": [
    "## **Generate financial summary from previous information and estimations (server)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def generate_financial_summary(raw_data):\n",
    "    \"\"\"\n",
    "    \n",
    "        Filter financial data and generate a formatted table with key metrics and interpretations.\n",
    "        \n",
    "        Parameters:\n",
    "            raw_data: dict - Dictionary containing raw financial metrics from web scraping and API calls.\n",
    "        Returns:\n",
    "            String containing a formatted table with filtered metrics, their values, and economic interpretations.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Definition of metrics to filter and their explanations\n",
    "    # Format: 'Original_Key': ('Readable_Name', 'Description/Importance')\n",
    "    metrics_map = {\n",
    "        'Price': ('Current Price', 'Current market value of the stock.'),\n",
    "        'Market Cap': ('Market Capitalization', 'Total size of the company in the market.'),\n",
    "        'Perf Year': ('Annual Performance', 'Percentage change in stock price over the last year.'),\n",
    "        'P/E': ('P/E Ratio', 'Price/Earnings. Indicates how expensive the stock is.'),\n",
    "        'Forward P/E': ('Forward P/E', 'Expected Price/Earnings ratio (lower values indicate potential improvement).'),\n",
    "        'Target Price': ('Target Price', 'Price that analysts expect within 12 months.'),\n",
    "        'consensus': ('Consensus', 'Average analyst opinion (Buy/Hold/Sell).'),\n",
    "        'targetHigh': ('Analysts Ceiling', 'The most optimistic price target recorded.'),\n",
    "        'EPS next Y': ('EPS Growth', 'Expected earnings growth for the next year.'),\n",
    "        'ROE': ('ROE (%)', 'Return on Equity. Measures efficiency of capital use.'),\n",
    "        'Debt/Eq': ('Debt/Capital', 'Leverage level. Low values indicate financial strength.'),\n",
    "        'Profit Margin': ('Profit Margin', 'Percentage of revenue converted to profit.'),\n",
    "        'RSI (14)': ('RSI Index', 'Indicates if the stock is overbought (>70) or oversold (<30).'),\n",
    "        'revenueAvg': ('Revenue 2030 (Est)', 'Long-term revenue projection according to API.'),\n",
    "        'numAnalystsEps': ('Number of Analysts', 'Quantity of experts covering this company.')\n",
    "    }\n",
    "\n",
    "    table_data = []\n",
    "    \n",
    "    # Filtering process (requirement for extra points)\n",
    "    for key, (label, description) in metrics_map.items():\n",
    "        value = raw_data.get(key, \"N/A\")\n",
    "        table_data.append([label, value, description])\n",
    "\n",
    "    # Table generation with tabulate\n",
    "    headers = [\"Metric\", \"Value\", \"Economic Interpretation\"]\n",
    "    \n",
    "    return tabulate(table_data, headers=headers, tablefmt='rounded_grid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1080a8",
   "metadata": {},
   "source": [
    "## **ERROR: Trying `BeautifulSoup & requests` to webscrape but raised 401 error (Not authorized)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362717f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimations(ticker):\n",
    "    # Ensure necessary imports\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    \n",
    "    url = f\"https://marketwatch.com/investing/stock/{ticker.lower()}/analystestimates\"\n",
    "    \n",
    "    headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "                \"Referrer\": \"https://google.com\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "    \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data for {ticker.upper()}. Status code: {response.status_code}\")\n",
    "            return {}\n",
    "    \n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        estimations = {}\n",
    "    \n",
    "        try:\n",
    "            tables = soup.find_all(\"table\", class_ = \"table value-pairs no-heading font--lato\")\n",
    "            \n",
    "            for i in range(2):\n",
    "                for row in tables[i].find_all(\"tr\"):\n",
    "                    cells = row.find_all(\"td\")\n",
    "                    if len(cells) >= 2:\n",
    "                        key = cells[0].get_text(strip=True)\n",
    "                        value = cells[1].get_text(strip=True)\n",
    "                        estimations[key] = value\n",
    "            \n",
    "            if estimations:\n",
    "                print(f\"Estimations for {ticker.upper()} extracted successfully\")\n",
    "            else:\n",
    "                print(f\"No estimations found for {ticker.upper()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while parsing the estimations: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while extracting estimations: {e}\")\n",
    "    \n",
    "    return estimations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b891b57",
   "metadata": {},
   "source": [
    "## **ERROR: Trying `Selenium` to webscrape but bot was detected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimations(driver, ticker):\n",
    "    # Ensure necessary imports\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "    # Go directly to the MarketWatch analyst estimates page for the ticker\n",
    "    url = f\"https://www.marketwatch.com/investing/stock/{ticker.lower()}/analystestimates\"\n",
    "    estimations = {}\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        print(\"Acceded to MarketWatch analyst estimates page\")\n",
    "        time.sleep(1) # Wait for the page to load\n",
    "        \n",
    "        # Cookie pop-up handling \n",
    "        try:\n",
    "            # Wait until appears the iframe containing the cookie button\n",
    "            iframe = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//iframe[contains(@id, 'sp_message_iframe_1396985')]\")))\n",
    "            \n",
    "            # Switch to the iframe\n",
    "            driver.switch_to.frame(iframe)\n",
    "            \n",
    "            # Wait for the cookie button to be clickable and click it\n",
    "            cookie_btn = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[text() = 'YES, I AGREE']\")))\n",
    "            cookie_btn.click()\n",
    "            \n",
    "            print(\"Cookie pop-up closed.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"No cookie pop-up found or already closed. Exception:\", e)\n",
    "\n",
    "        # Ensure we are at the main content\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "        # Wait for the estimation tables to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"table value-pairs no-heading font--lato\")))\n",
    "\n",
    "        # Find all the estimation tables\n",
    "        tables = driver.find_elements(By.CLASS_NAME, \"table value-pairs no-heading font--lato\")\n",
    "        \n",
    "        # For the first two tables, extract key-value pairs\n",
    "        for i in range(2):\n",
    "            rows = tables[i].find_elements(By.TAG_NAME, \"tr\")\n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if len(cells) == 2:\n",
    "                    key = cells[0].text.strip()\n",
    "                    value = cells[1].text.strip()\n",
    "                    estimations[key] = value\n",
    "        \n",
    "        # If no estimations were found, log a warning\n",
    "        if not estimations:\n",
    "            print(f\"Warning: No data scraped for {ticker}\")\n",
    "        else:\n",
    "            print(f\"Successfully scraped analyst data for {ticker}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping MarketWatch for {ticker}: {e}\")\n",
    "\n",
    "    return estimations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
